project_name: "Synesthesia_Project"

paths:
  # Generated dataset path
  synthetic_dir: "./data/synthetic"
  # Model checkpoints path
  checkpoint_dir: "./data/checkpoints"
  # csv file containing metadata (image/audio paths, labels, etc.)
  metadata_file: "metadata.csv"

model:
  # Image encoder model (CLIP)
  image_encoder: "openai/clip-vit-base-patch32"
  # Audio encoder model (CLAP)
  audio_encoder: "laion/clap-htsat-unfused"
  # Adaptator architecture
  input_dim: 768  # CLIP output
  hidden_dim: 768
  output_dim: 512 # CLAP output

training:
  device: "cpu" # or "cpu" if no GPU available
  batch_size: 2
  learning_rate: 1e-4
  epochs: 2
  save_every: 1 # Save model checkpoint every N epochs

generation:
  # Number of synthetic samples to generate
  num_samples: 5
  steps: 20 # Number of inference steps for image generation (default: 20)
  guidance_scale: 7.5 # How strongly the model should follow the prompt (default: 7.5)