import os
import yaml
import torch
import numpy as np
from PIL import Image
from transformers import CLIPImageProcessor
from diffusers import AudioLDMPipeline

# Import de ton mod√®le
from src.models.synesthesia_model import SynesthesiaModel

class SynesthesiaPipeline:
    """
    Pipeline End-to-End pour g√©n√©rer du son √† partir d'une image.
    √Ä instancier une seule fois dans un Notebook.
    """
    def __init__(self, checkpoint_path, config_path="configs/config.yaml"):
        # 1. Configuration & Device
        with open(config_path, "r") as f:
            self.config = yaml.safe_load(f)
            
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"‚öôÔ∏è Initializing Synesthesia Pipeline on {self.device}...")

        # 2. Charger le Cerveau Visuel (Ton mod√®le)
        print("üëÅÔ∏è Loading Vision Model...")
        self.model = SynesthesiaModel(self.config).to(self.device)
        self.model.load_state_dict(
            torch.load(checkpoint_path, map_location=self.device, weights_only=True), 
            strict=False
        )
        self.model.eval()
        self.image_processor = CLIPImageProcessor.from_pretrained(self.config['model']['image_encoder'])

        # 3. Charger le G√©n√©rateur Audio (AudioLDM)
        print("üéπ Loading Audio Synthesizer (AudioLDM)...")
        self.audio_pipe = AudioLDMPipeline.from_pretrained("cvssp/audioldm-s-full-v2", torch_dtype=torch.float32)
        self.audio_pipe = self.audio_pipe.to(self.device)
        
        if self.device == "cpu":
            self.audio_pipe.enable_attention_slicing()
            
        print("‚úÖ Pipeline ready!")

    def generate(self, image_input, duration=5.0, steps=20):
        """
        G√©n√®re l'audio.
        image_input: peut √™tre un chemin (str) ou directement un objet PIL Image.
        Retourne: un tuple (audio_array, sample_rate) pr√™t √† √™tre lu.
        """
        # A. Pr√©parer l'image
        if isinstance(image_input, str):
            image = Image.open(image_input).convert('RGB')
        else:
            image = image_input.convert('RGB')

        # B. Pr√©dire le vecteur latent
        with torch.no_grad():
            inputs = self.image_processor(images=image, return_tensors="pt").to(self.device)
            predicted_embedding = self.model(inputs['pixel_values'])
            
            # Normalisation et formatage pour AudioLDM
            predicted_embedding = predicted_embedding / predicted_embedding.norm(dim=-1, keepdim=True)
            prompt_embeds = predicted_embedding
            negative_embeds = torch.zeros_like(prompt_embeds)

        # C. G√©n√©rer le son
        audio_output = self.audio_pipe(
            prompt_embeds=prompt_embeds,
            negative_prompt_embeds=negative_embeds,
            num_inference_steps=steps, 
            audio_length_in_s=duration
        ).audios[0]

        # AudioLDM g√©n√®re toujours en 16000 Hz
        return audio_output, 16000